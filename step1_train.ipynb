{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/workspace/Documents')  ### remove this if not needed!\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from tqdm import tqdm \n",
    "import random\n",
    "from pathlib import Path\n",
    "import nibabel as nb\n",
    "import time\n",
    "\n",
    "import argparse\n",
    "from einops import rearrange\n",
    "from natsort import natsorted\n",
    "from madgrad import MADGRAD\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    " \n",
    "from original_SAM.utils.model_util import *\n",
    "from original_SAM.segment_anything.model import build_model \n",
    "from original_SAM.utils.save_utils import *\n",
    "from original_SAM.utils.config_util import Config\n",
    "from original_SAM.utils.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "\n",
    "# from original_SAM.train_engine import train_loop\n",
    "\n",
    "import original_SAM.dataset.build_datasets as build_datasets\n",
    "import original_SAM.functions_collection as ff\n",
    "import original_SAM.get_args_parser as get_args_parser\n",
    "\n",
    "main_path = '/mnt/camca_NAS/SAM_for_CMR/'  # replace with your own path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define parameters for this experiment\n",
    "The full setting can be find in ```get_args_parser.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set experiment-specific parameters\n",
    "trial_name = 'original_SAM_trial' \n",
    "\n",
    "output_dir = os.path.join(main_path, 'example_data_original_sam/models', trial_name)\n",
    "ff.make_folder([os.path.join(main_path, 'example_data_original_sam/models'), output_dir])\n",
    "\n",
    "pretrained_model = None # define your pre-trained model if any\n",
    "start_epoch = 1\n",
    "total_training_epochs = 100 # define total number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the original SAM model\n",
    "original_sam = os.path.join( main_path, 'models/pretrained_sam/sam_vit_b_01ec64.pth') \n",
    "\n",
    "# define the number of segmented classes ## important\n",
    "num_classes = 3\n",
    "\n",
    "args = get_args_parser.get_args_parser(num_classes = num_classes,\n",
    "                                       vit_type = \"vit_b\",\n",
    "                                       pretrained_model = pretrained_model, \n",
    "                                       original_sam = original_sam, \n",
    "                                       start_epoch = start_epoch, \n",
    "                                       total_training_epochs = total_training_epochs)\n",
    "args = args.parse_args([])\n",
    "\n",
    "# some other settings\n",
    "cfg = Config(args.config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define SAX training data\n",
    "patient_list_file_sax = os.path.join(main_path,'example_data_original_sam/Patient_list/patient_list.xlsx')\n",
    "patient_index_list = np.arange(0,1,1)\n",
    "dataset_train_sax = build_datasets.build_dataset(\n",
    "        args,\n",
    "        patient_list_file = patient_list_file_sax, \n",
    "        index_list = patient_index_list, \n",
    "        shuffle = True, \n",
    "        augment = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load pre-trained SAM model (freeze SAM modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.vit_type =  vit_b\n",
      "new training\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_243106/2734249530.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    }
   ],
   "source": [
    "# set model\n",
    "model = build_model(args, device)\n",
    "\n",
    "# # set freezed and trainable keys\n",
    "train_keys = []\n",
    "freezed_keys = []\n",
    "        \n",
    "# load pretrained sam model vit_b\n",
    "if args.model_type.startswith(\"sam\"):\n",
    "    if args.resume.endswith(\".pth\"):\n",
    "        print('args.vit_type = ', args.vit_type)\n",
    "        with open(args.resume, \"rb\") as f:\n",
    "            state_dict = torch.load(f)\n",
    "        try:\n",
    "            model.load_state_dict(state_dict)\n",
    "        except:\n",
    "            if args.vit_type == \"vit_h\":\n",
    "                new_state_dict = load_from(model, state_dict, args.img_size,  16, [7, 15, 23, 31])\n",
    "               \n",
    "            model.load_state_dict(new_state_dict)\n",
    "        \n",
    "        # # freeze original SAM layers\n",
    "        # freeze_list = [ \"norm1\", \"attn\" , \"mlp\", \"norm2\"]  \n",
    "                \n",
    "        for n, value in model.named_parameters():\n",
    "            value.requires_grad = True\n",
    "            # if any(substring in n for substring in freeze_list):\n",
    "            #     freezed_keys.append(n)\n",
    "            #     value.requires_grad = False\n",
    "            # else:\n",
    "            #     train_keys.append(n)\n",
    "            #     value.requires_grad = True\n",
    "\n",
    "## Select optimization method\n",
    "optimizer = MADGRAD(model.parameters(), lr=args.lr) # momentum=,weight_decay=,eps=)\n",
    "        \n",
    "# Continue training model\n",
    "if args.pretrained_model is not None:\n",
    "    if os.path.exists(args.pretrained_model):\n",
    "        print('loading pretrained model : ', args.pretrained_model)\n",
    "        args.resume = args.pretrained_model\n",
    "        finetune_checkpoint = torch.load(args.pretrained_model)\n",
    "        model.load_state_dict(finetune_checkpoint[\"model\"])\n",
    "        optimizer.load_state_dict(finetune_checkpoint[\"optimizer\"])\n",
    "        torch.cuda.empty_cache()\n",
    "else:\n",
    "    print('new training\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
